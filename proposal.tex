%%
%% Author: fatemeh
%% 2019-11-25
%%

%\chapter{Going Beyond Limitation in Sequence Analyses}
\chapter{Ongoing and Proposed Work}

The following proposed work includes ongoing and future projects.

\section{A scalable and Dynamic Search Index via Log-Structured MST Mantis Merge Tree}

As showed in~\ref{chap:mantis}, Mantis, our large sequencing search index
was orders of magnitude faster than the state-of-the-art tools at that time (SBT-like structures)
although requiring a smaller fraction of memory compared to them.
We were able to scale Mantis to $10,000$ raw sequencing samples in our next work
by reformatting the color representation from bit-vectors to a minimum spanning tree
linking colors based on the minimum required flipping bits
to construct one color bitvectors from other colors.
However, even until this point the problem of scalability is not fully addressed
and resolved.
Comparing the original Mantis's representation, which we call ``classic'' mantis
with the MST-based representation, we observed a flipping point in the scalability bottleneck
from the color information to the \kmer filter representation, \cqf
in larger datasets.
In addition to that, the necessity of having the color bit matrix as the prerequisite
to construct the MST over the color bitvectors, is another obstacle
to achieving the goal of scalability.

In this proposal, we, I in collaboration with the Mantis team
~\footnote{Professors Rob Patro, Rob Johnson, Micheal Ferdman, Micheal Bender, Prashant Pandey, Jamshed Khan,
and Sergey Madaminov} introduce an algorithm to merge both “classic”
and the MST-based mantis data structures. The later, gives us the advantage of skipping
construction of the intermediate large color bit matrices by instead, directly merging
the MST representations for its two smaller constituent Mantises.
We also require to merge the two filters along with the color representations.
However, since the keys are sorted in the two \cqfs, the merging of them
is conceptually equal to merging two sorted list.
The only challenge in that process is keeping the memory usage low,
for which we propose an intelligent multi-pass algorithm that avoids holding
all data for the input and output structures in memory at once.
In fact, based on the proposed strategy, the memory is constant and
practically bounded by 10GB for all dataset sizes considered in our initial experiments.

To allow the merging of MST-based mantis data structures,
using the previously introduced LRU-cache to speed up fetching the color bitvectors
was not sufficient enough as the nature of the queries in merging two Mantises
is different from the typical queries in a biological sequence search.
Therefore, we are facing a new challenge in how to optimally plan the caching
to help the query time. We plan to tackle the cache planning problem and
propose an efficient heuristic that allows constructing a fixed cache of MST nodes
so that, in expectation, no query is too far from a cached node.

Another deficiency of the previously proposed structure, Mantis,
is its lack of being dynamically updatable in the sense that one cannot
add a new raw sequencing sample to the index without accepting the burden of
reconstructing Mantis over all the samples again including the new one.
Given the ability to efficiently merge both classic and MST-based mantis data structures,
we can think of one way to dynamize the structure as using log-structured merge trees.

These two goals together allow creating a mantis data structure that can be constructed in small memory,
and which is also able to grow dynamically at the resolution of individual experiments.


\section{Cedar, A Fast High Accuracy Tool for Metagenomic Abundance Estimation}
One of the main challenges in metagenomic abundance estimation is the large number of
false positives, resulting in reporting abundance values for species
which are not even present in the sequenced sample.
I along with my teammate Mohsen propose ``Cedar'',
a complete pipeline from aligning metagenomic reads to estimating abundances at each taxonomic level
which is sometimes also called profiling.
The proposal follows the same pipeline as many previous microbiome profiling tools~\cite{},
dividing the task of profiling into two independent sections of alignment and abundance estimation.
We use Puffalign, our recently developed time and space efficient alignment tool
based on the \pufferfish index for the first step and Cedar for the second part.
I should note that both of these tools can be used in other sequence analyzing pipelines
in combination with other tools.

In many metagenomic quantification tasks,
considering the depth of the high throughput sequencing techniques
and high amount of sequence identity across genomes,
low abundant sequences or taxa are usually discarded from the quantification results.
One solution proposed to avoid reporting a lot of false positive genomes as abundant
is having a post-filtering step
to discard references with abundances smaller than a predefined cutoff value
or discard all the multi-mapped reads~\cite{bracken,karp}.
This post-filtering, induces losing the reads that were mapped to those discarded references.
However, we believe if we keep these reads and distribute them among the remaining references,
this can help better estimate the counts of the remaining references
based on the following common knowledge bases:
\begin{enumerate}
    \item Each read is supposed to be the output of a sequencing process and contains information,
    so discarding the read is equal to losing information.
    \item Each read can be multi-mapped to the truth and some other references
    in case of no errors in the sequencing.
    \item The sequencing error rate is very low and if the mapping tool allows sub-optimal mappings,
    we still can rely on the fact that we have the truth
    among the set of references for a multi-mapped read.
    \item In high throughput sequencing, there is a very low probability for having low abundant references.
\end{enumerate}

Considering the above facts, keeping the reads and
distributing them among the remaining references
can help to make the final count distribution more skewed and closer to the truth
as we are using the information coming from more reads
which is the main motivation for developing ``Cedar''.

Cedar relies on an Expectation-Maximization algorithm (EM) for estimating abundances
and avoids reporting false positives by an iterative filtering step during the EM procedure.
Iterative thresholding is the solution we propose tackling the problem of
``reference sparsifying without the loss of reads''.
Through out the EM process, at the beginning of each iteration,
we go over the following four steps of thresholding:
\begin{enumerate}
    \item~\textbf{Mark References as Potentially Removable (\abpt)}:
    We go over all the references and mark a reference as potentially removable
    if it has a count smaller than or equal to the given cutoff.
    Our main goal is to discard as many references as possible
    from the list of potentially removable references without losing any reads.
    \item~\textbf{Remove Safe \abpt References Immediately}:
    A read will be lost if all the references that are equivalent over this read are discarded
    since the \eq that the read belongs to is defined by no references
    and hence the read is mapped no where.
    Therefore, out of the list of the potentially removable references,
    one can be safely removed if in each of the \eqs it belongs to there exists at least one reference
    that does not belong to this list.
    \item~\textbf{Solve the Set Cover Problem for Unsafe \abpt References}:
    For any reference $g_i$ in the remaining set of \abpt references,
    there exists at least one \eq that all its references are \abpt including $g_i$.
    We call such \eqs, \textit{critical \eqs} and according to our goal
    we want to rescue minimum number of \abpt references that can make all critical \eqs
    so that all the reads in them remain valid.
    The definition of this problem is equivalent to the set cover problem.
    Each reference represents a set of critical \eqs (those that it is part of)
    and we want to select minimum number of sets (e.g. references)
    that can explain all the set members (e.g. critical \eqs).
    As it is known, solving the set cover problem is np-hard~\cite{},
    therefore we choose the greedy approach to approximate the output list of references.
    The remaining elements (i.g. reads) at the end of the set-cover process are those that cannot be removed.
    For the rest of the list, we can safely remove them without losing any reads.
    \item~\textbf{Update Equivalence Classes}:
    In the last step, we need to update all \eqs that have lost any reference
    and the weights of the references for the next iterations of EM.
\end{enumerate}

Finally, we plan to compare Cedar to the state-of-the-art metagenomic abundance estimation tools
such as Bracken~\cite{} and Karp~\cite{}.


\chapter{Timeline}

\renewcommand{\arraystretch}{2.0}
\begin{center}
    \begin{tabularx}{\linewidth}{|X|c|}
        \hline
        Development and Experiment Design for Dynamic and Hierarchically Mergable Mantis & 2 months \\
        Experiment Design and Extensive tests for Puffalign & 2 months \\
        Development, and Experiment Design of Cedar for Metagenomic Abundance Estimation & 3-4 months \\
        \hline
        \textbf{TOTAL} & 7-8 months \\
        \hline
    \end{tabularx}

\end{center}


Paper deadline goals:

\begin{itemize}
    \item ISMB, January 2019: Dynamic Mantis allows merging new mRNA sequence samples into the index over
    thousands of experiments
    \item Bioinformatics, April 2019: Puffalign, a space and time-efficient aligner on top of the Pufferfish index
    \item Bioinformatics, July 2019: Cedar,
\end{itemize}

